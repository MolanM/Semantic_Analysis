{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Importing pickle for opening the files\n",
    "In pickle files we have saved encoded versions of our text files. One file contains incels comment, 2nd one contains reddit comments and the 3rd one contains hate speech. Those files are encoded with bert and electra. Encoding process for the reddit data is done in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1651753607612
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Importing tensorflow\n",
    "Tensorflow is used for creating the models and also we used tensorflow_hub to open elektra and bert from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1651753646262
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Importing time\n",
    "Just to have more informations from our function we used time package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1651753882991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Importing pandas\n",
    "This will help us in opening existing CSV file that contains reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1651753889659
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Loading the pickle files\n",
    "Next three cell will load all pickle files that we have. Here we printed the length of the pickle files just to check if everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1650535548848
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "with open('encoded_c1_incels.pkl', 'rb') as outp:\n",
    "    rr = pickle.load(outp)\n",
    "\n",
    "print(len(rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1650535549233
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "with open('encoded_baseline.pkl', 'rb') as outp:\n",
    "    ba = pickle.load(outp)\n",
    "\n",
    "print(len(ba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Creating datasets for validation and testing\n",
    "Here we used existing data and used some of it's parts for 2 different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1650536250506
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "149\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "# test set: 500 class 0, 500 class 1\n",
    "#in chunks of 50, 500 = 50*10\n",
    "chunks = 10\n",
    "\n",
    "ind_b = [i for i in range(len(ba))]\n",
    "ind_r = [i for i in range(len(rr))]\n",
    "shuffle(ind_b)\n",
    "shuffle(ind_r)\n",
    "\n",
    "\n",
    "train0 = [ba[i] for i in ind_b[:-chunks]]\n",
    "train1 = [rr[i] for i in ind_r[:-chunks]]\n",
    "\n",
    "test0 = [ba[i] for i in ind_b[-chunks:]]\n",
    "test1 = [rr[i] for i in ind_r[-chunks:]]\n",
    "\n",
    "val0 = test0[:len(test0)//2]\n",
    "test0 = test0[len(test0)//2:]\n",
    "\n",
    "val1 = test1[:len(test1)//2]\n",
    "test1 = test1[len(test1)//2:]\n",
    "\n",
    "for i in (train0, train1, test0, test1, val0, val1): print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Importing numpy\n",
    "We used numpy for reshaping of our data arrays and also to create some supportive arrays (like the array with all zeros or all ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1650536478115
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## More on data preprocessing\n",
    "So here we created the function that will concatenate, shuffle out data. Also we used one-hot-encoder in order to transform text into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1650536479585
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(ba,rr):\n",
    "    cl0 = tf.keras.backend.concatenate(\n",
    "        ba,\n",
    "        axis=0\n",
    "    )\n",
    "    cl1 = tf.keras.backend.concatenate(\n",
    "        rr,\n",
    "        axis=0\n",
    "    )\n",
    "    train = tf.keras.backend.concatenate(\n",
    "        [cl0,cl1],\n",
    "        axis=0\n",
    "    )\n",
    "    y = np.concatenate([np.zeros(cl0.shape[0]), np.ones(cl1.shape[0])]).astype(int)\n",
    "\n",
    "    #shufle\n",
    "    ind_list = [i for i in range(y.shape[0])]\n",
    "    shuffle(ind_list)\n",
    "    train_new = np.array([train[i] for i in ind_list])\n",
    "    y_new = np.array([y[i] for i in ind_list])\n",
    "\n",
    "    #one-hot\n",
    "    train_y = np.zeros((y_new.size, y_new.max()+1))\n",
    "    train_y[np.arange(y_new.size),y_new] = 1\n",
    "    train_y.shape\n",
    "    \n",
    "    return(train_new, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train =  preprocess(train0, train1)\n",
    "X_val, y_val = preprocess(val0,val1)\n",
    "X_test, y_test =  preprocess(test0, test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Importing sklearn and keras\n",
    "Sklearn is the best package for maching learning in python. We used so many of it's functions to create fine models and also to do osme training, testing, etc.\n",
    "From keras we only needed function for loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1650538501578
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from os import makedirs\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1650538624263
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        #print(yhat.shape)\n",
    "        # stack predictions into [rows, members, probabilities]d\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    return stackX.reshape(stackX.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gather": {
     "logged": 1650538611750
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    \n",
    "    #model = LogisticRegression()\n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    model.fit(stackedX, inputy.argmax(axis = -1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gather": {
     "logged": 1650538635427
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gather": {
     "logged": 1650538558276
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'new_models_50/model_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gather": {
     "logged": 1650538581865
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded new_models_50/model_1.h5\n",
      ">loaded new_models_50/model_2.h5\n",
      ">loaded new_models_50/model_3.h5\n",
      ">loaded new_models_50/model_4.h5\n",
      ">loaded new_models_50/model_5.h5\n",
      ">loaded new_models_50/model_6.h5\n",
      ">loaded new_models_50/model_7.h5\n",
      ">loaded new_models_50/model_8.h5\n",
      ">loaded new_models_50/model_9.h5\n",
      ">loaded new_models_50/model_10.h5\n",
      "Loaded 10 models\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "n_members = 10\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "gather": {
     "logged": 1650545198737
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1024), (500, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "gather": {
     "logged": 1650538640203
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.910\n",
      "Model Accuracy: 0.994\n",
      "Model Accuracy: 0.992\n",
      "Model Accuracy: 0.924\n",
      "Model Accuracy: 0.926\n",
      "Model Accuracy: 0.994\n",
      "Model Accuracy: 0.976\n",
      "Model Accuracy: 0.992\n",
      "Model Accuracy: 0.930\n",
      "Model Accuracy: 0.994\n",
      "Stacked Test Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "#on val set\n",
    "for model in members:\n",
    "    _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "\n",
    "model_s = fit_stacked_model(members, X_val, y_val)\n",
    "# evaluate model on test set\n",
    "\n",
    "yhat = stacked_prediction(members, model_s, X_val)\n",
    "acc = accuracy_score(y_val.argmax(axis = -1), yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "gather": {
     "logged": 1650544154590
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1024), (500, 2))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "gather": {
     "logged": 1650538648411
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.916\n",
      "Model Accuracy: 0.976\n",
      "Model Accuracy: 0.980\n",
      "Model Accuracy: 0.926\n",
      "Model Accuracy: 0.922\n",
      "Model Accuracy: 0.978\n",
      "Model Accuracy: 0.972\n",
      "Model Accuracy: 0.976\n",
      "Model Accuracy: 0.930\n",
      "Model Accuracy: 0.974\n",
      "Stacked Test Accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "#on test set\n",
    "for model in members:\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "    \n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model_s, X_test)\n",
    "acc = accuracy_score(y_test.argmax(axis = -1), yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "txt_gpu2",
   "language": "python",
   "name": "txt_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
